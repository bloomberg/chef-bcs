{
  "name": "vagrant",
  "json_class": "Chef::Environment",
  "description": "Vagrant test environment",
  "NOTE": "NOTE key is ONLY for documenting this json data since json does not allow for comments. This data file is for using Vagrant instead of PXE booting like in production on bare-metal nodes.",
  "cookbook_versions": {},
  "chef_type": "environment",
  "override_attributes": {
    "chef-bcs": {
      "environment": "vagrant",
      "repo": {
        "NOTE": "Pins the current version of ceph or any other packaged in the array at the moment of install.",
        "packages": [
          {"name": "ceph", "version": "10.2.0-0.el7", "pin": true}
        ]
      },
      "bootstrap": {
        "name": "ceph-bootstrap",
        "env": "/home/vagrant/chef-bcs/environments",
        "interfaces": [
          {"name": "enp0s8", "ip": "10.0.100.20", "netmask": "255.255.255.0", "gateway": ""},
          {"name": "enp0s9", "ip": "10.121.2.2", "netmask": "255.255.255.0", "gateway": ""}
        ]
      },
      "adc": {
        "tag": "ceph-adc",
        "interface": "enp0s8",
        "stats": {
          "enable": true,
          "user": "haproxy",
          "passwd": "haproxy",
          "port": 1936
        },
        "bond": {
          "enable": false,
          "name": "bond0",
          "type": "Bond",
          "mtu": 1500,
          "interfaces": ["enp0s8", "enp0s9"],
          "options": "mode=4 miimon=100 xmit_hash_policy=layer3+4",
          "vbox_options": "mode=1 miimon=100 fail_over_mac=1",
          "nm_controlled": "no"
        },
        "connections": {
          "max": 1000,
          "balance": "roundrobin"
        },
        "ssl": {
          "path": "/etc/ssl/private"
        },
        "vip": {
          "prefix": "10.121.16.16",
          "netmask": "255.255.255.240",
          "cidr": 28,
          "port": {
            "ssl": 443,
            "non_ssl": 80
          }
        },
        "vips": [
          {"name": "prod", "ip": "10.121.16.17", "cidr": 28, "interface": "enp0s8", "ssl": false, "cert": "s3.vagrant.ceph.example.com.crt", "ssl_files": [], "url": "s3.vagrant.prod.ceph.example.com"},
          {"name": "dev", "ip": "10.121.16.18", "cidr": 28, "interface": "enp0s8", "ssl": false, "cert": "s3.vagrant.ceph.example.com.crt", "ssl_files": [], "url": "s3.vagrant.dev.ceph.example.com"}
        ],
        "NOTE-VIP": "VIPS are 10.121.16.16/28 range (.17 - .30). Advertised via BGP",
        "bgp": {
          "enable": false,
          "asn": 65001,
          "interface": "enp0s8",
          "roles": [
            {"name": "ceph-vm1", "role": "primary"},
            {"name": "ceph-vm2", "role": "secondary"}
          ],
          "peers": [
            {"name": "bgp_peer1", "label": "Peer 1", "ip": "10.121.16.8"}
          ]
        },
        "backend": {
          "servers": [
            {"name": "ceph-vm1", "instance": "dev", "weight": 6, "port": 8080, "options": "check inter 2s rise 2 fall 3"},
            {"name": "ceph-vm1", "instance": "prod", "weight": 6, "port": 8081, "options": "check inter 2s rise 2 fall 3"},
            {"name": "ceph-vm2", "instance": "dev", "weight": 3, "port": 8080, "options": "check inter 2s rise 2 fall 3"},
            {"name": "ceph-vm2", "instance": "prod", "weight": 3, "port": 8081, "options": "check inter 2s rise 2 fall 3"}
          ],
          "NOTE-BES": "If not Federated then leave 'instance' empty and have only server name unique in the list instead of the way Federated requires the instance name to match the VIP name."
        }
      },
      "keepalived": {
        "passwd": "keepalived",
        "checks": true,
        "servers": [
          {"name": "ceph-vm1", "weight": 5, "interface": "enp0s8", "priority": 98, "router-id": 25},
          {"name": "ceph-vm2", "weight": 5, "interface": "enp0s8", "priority": 100, "router-id": 50},
          {"name": "ceph-vm3", "weight": 5, "interface": "enp0s8", "priority": 99, "router-id": 75}
        ]
      },
      "chef": {
        "owner": "vagrant",
        "group": "vagrant"
      },
      "security": {
        "sshd": {
          "permit_root_login": "no",
          "login_grace_time": "2m",
          "max_auth_tries": 6,
          "max_sessions": 10,
          "banner": "/etc/banner"
        },
        "firewall": {
          "interfaces": [
            {
              "name": "public",
              "ports": [
                {"role": "ceph-bootstrap", "open": [{"port": 123, "protocol": "udp"}, {"port": 80, "protocol": "tcp"}, {"port": 443, "protocol": "tcp"}, {"port": 67, "protocol": "udp"}, {"port": 69, "protocol": "udp"}, {"port": 21, "protocol": "tcp"}, {"port": 4011, "protocol": "udp"}, {"port": 53, "protocol": "udp"}], "ranges": [{"start": 25150, "end": 25152, "protocol": "tcp"}]},
                {"role": "ceph-mon", "open": [{"port": 6789, "protocol": "tcp"}], "ranges": [{"start": 0, "end": 0, "protocol": "tcp"}]},
                {"role": "ceph-osd", "open": [], "ranges": [{"start": 6800, "end": 6872, "protocol": "tcp"}]},
                {"role": "ceph-rgw", "open": [8080], "ranges": [{"start": 0, "end": 0, "protocol": "tcp"}]},
                {"role": "ceph-restapi", "open": [{"port": 5080, "protocol": "tcp"}], "ranges": [{"start": 0, "end": 0, "protocol": "tcp"}]},
                {"role": "ceph-admin", "open": [], "ranges": [{"start": 0, "end": 0, "protocol": "tcp"}]},
                {"role": "ceph-mds", "open": [], "ranges": [{"start": 6800, "end": 6872, "protocol": "tcp"}]},
                {"role": "ceph-rbd", "open": [], "ranges": [{"start": 0, "end": 0, "protocol": "tcp"}]},
                {"role": "haproxy", "open": [{"port": 80, "protocol": "tcp"}, {"port": 443, "protocol": "tcp"},{"port": 1936, "protocol": "tcp"}], "ranges": [{"start": 0, "end": 0, "protocol": "tcp"}]},
                {"role": "keepalived", "open": [{"port": 112, "protocol": "tcp"}], "ranges": [{"start": 0, "end": 0, "protocol": "tcp"}]}
              ]
            },
            {
              "name": "cluster",
              "ports": [
                {"role": "ceph-osd", "open": [], "ranges": [{"start": 6800, "end": 6872, "protocol": "tcp"}]}
              ]
            }
          ],
        "NOTE1": "NOTE: Firewall open ports are accumulative for each node based on it's role. Role must match ceph-chef tags.",
        "NOTE2": "NOTE: Range start = 0 then range is skipped else put in exact ranges.",
        "NOTE3": "NOTE: OSDs start at 6800 and each OSD uses at least 3 ports. The end number should be high enough to account for this. MDS should match OSD.",
        "NOTE4": "NOTE: If you run multiple instances of RGW then keep the port count in mind."
        }
      },
      "system": {
        "sysctl": [
          "kernel.pid_max=4194303",
          "fs.file-max=26234859"
        ]
      },
      "ipmi": {
        "user": "vbox",
        "passwd": "$6$Salt$xvQkYaQ4urNWmnjpinAZSR/ZOaRy/aacKh4j18ayq/.mswLqleFZI5zaD1BCg2Fdzy1BjpBv9VIgVgt6YoA8T0",
        "NOTE": "password of vbox is: $6$Salt$xvQkYaQ4urNWmnjpinAZSR/ZOaRy/aacKh4j18ayq/.mswLqleFZI5zaD1BCg2Fdzy1BjpBv9VIgVgt6YoA8T0"
      },
      "cobbler": {
        "web_user": "cobbler",
        "pxe_interface": "enp0s3",
        "server": "10.0.100.20",
        "kickstart": {
          "NOTE": "password vagrant is: $6$Salt$6AyUczFy6SgV8A2wKAKfA9drpzrUsTGPJ3QjcWBbgS97BxBO.C7ZcBFALRiRkKfi9x8MK2SHet38BCQWS9LsR/",
          "root": {
            "passwd": "$6$Salt$6AyUczFy6SgV8A2wKAKfA9drpzrUsTGPJ3QjcWBbgS97BxBO.C7ZcBFALRiRkKfi9x8MK2SHet38BCQWS9LsR/",
            "passwd_type": "--iscrypted",
            "key": "ceph_bootstrap.pub"
          },
          "file": {
            "osd": "bcs_node_rhel_osd.ks",
            "nonosd": "bcs_node_rhel_nonosd.ks"
          },
          "bootloader": {
            "passwd": "",
            "passwd_type": ""
          },
          "users": [
            {
              "name": "vagrant",
              "passwd": "$6$Salt$6AyUczFy6SgV8A2wKAKfA9drpzrUsTGPJ3QjcWBbgS97BxBO.C7ZcBFALRiRkKfi9x8MK2SHet38BCQWS9LsR/",
              "passwd_type": "--iscrypted",
              "key": "ceph_bootstrap.pub",
              "shell": "/bin/bash",
              "comment": "Vagrant user",
              "groups": "wheel",
              "sudo": true
            },
            {
              "name": "operations",
              "passwd": "$6$Salt$3xxLPT099nzTbWkOS3CPNcar/zSLQ8BEgZdJk/AOkOb4V80sPepbraWcvrJvEEu6PswpKUw1WodWeiqRo1bw2/",
              "passwd_type": "--iscrypted",
              "key": "ceph_bootstrap.pub",
              "shell": "/bin/bash",
              "comment": "Operations user",
              "groups": "wheel",
              "sudo": true
            }
          ]
        },
        "profiles": [
          {"name": "ceph_osd_node", "file_type": "osd", "comment": "OSD type nodes either dedicated OSD or converged with other services like MON and RGW."},
          {"name": "ceph_non_osd_node", "file_type": "nonosd", "comment": "NON-OSD type nodes. Services like MON, RGW or MDS."}
        ],
        "servers": [
          {
            "name": "ceph-bootstrap",
            "roles": ["bootstrap", "adc"],
            "profile": "",
            "network": {
              "public": {
                "interface": "enp0s3",
                "mac": "08:00:27:E3:84:01",
                "ip": "10.0.100.20",
                "netmask": "255.255.255.0",
                "gateway": "10.0.100.2",
                "mtu": 1500
              },
              "cluster": {
                "interface": "enp0s8",
                "mac": "08:00:27:3E:CB:B0",
                "ip": "10.121.2.3",
                "netmask": "255.255.255.0",
                "gateway": "10.121.2.2",
                "mtu": 1500
              }
            }
          },
          {
            "name": "ceph-vm1",
            "roles": ["mon", "rgw", "osd", "adc", "admin"],
            "profile": "ceph_non_osd_node",
            "network": {
              "public": {
                "interface": "enp0s3",
                "mac": "08:00:27:E3:84:01",
                "ip": "10.0.100.21",
                "netmask": "255.255.255.0",
                "gateway": "10.0.100.2",
                "mtu": 1500
              },
              "cluster": {
                "interface": "enp0s8",
                "mac": "08:00:27:3E:CB:B0",
                "ip": "10.121.2.3",
                "netmask": "255.255.255.0",
                "gateway": "10.121.2.2",
                "mtu": 1500
              }
            }
          },
          {
            "name": "ceph-vm2",
            "roles": ["mon", "rgw", "osd", "adc", "admin"],
            "profile": "ceph_non_osd_node",
            "network": {
              "public": {
                "interface": "enp0s3",
                "mac": "08:00:27:EB:EC:61",
                "ip": "10.0.100.22",
                "netmask": "255.255.255.0",
                "gateway": "10.0.100.2",
                "mtu": 1500
              },
              "cluster": {
                "interface": "enp0s8",
                "mac": "08:00:27:C5:9F:40",
                "ip": "10.121.2.4",
                "netmask": "255.255.255.0",
                "gateway": "10.121.2.2",
                "mtu": 1500
              }
            }
          },
          {
            "name": "ceph-vm3",
            "roles": ["mon", "rgw", "osd", "adc", "admin"],
            "profile": "ceph_non_osd_node",
            "network": {
              "public": {
                "interface": "enp0s3",
                "mac": "08:00:27:8B:9F:3F",
                "ip": "10.0.100.23",
                "netmask": "255.255.255.0",
                "gateway": "10.0.100.2",
                "mtu": 1500
              },
              "cluster": {
                "interface": "enp0s8",
                "mac": "08:00:27:1B:3B:E7",
                "ip": "10.121.2.5",
                "netmask": "255.255.255.0",
                "gateway": "10.121.2.2",
                "mtu": 1500
              }
            }
          }
        ],
        "dhcp": {
          "shared_network": "bcs",
          "single": {
            "netmask": "255.255.255.0",
            "gateway": "10.0.100.2"
          },
          "subnets":[
            {"subnet": "10.0.100.0", "tag": "rack1", "dhcp_range": ["10.0.100.20", "10.0.100.30"], "netmask": "255.255.255.0", "router": "10.0.100.2"}
          ]
         },
        "NOTE1": "NOTE: Each subnet represents a routable rack so dhcp will need to manage each subnet with the TOR using IP-helper for dhcp requests by nodes in the given rack.",
        "NOTE2": "NOTE: You could just have one subnet for a single L2 span set of racks.",
        "NOTE3": "NOTE: /27 for subnet mask of each rack. DNS could be added to each subnet entry above but the global DNS entry below is good enough for this.",
        "NOTE4": "IF raid = true  or false then set partition_option --only-use=sda because Vagrant sets the first drive as an OS and modifications to that will cause OSDs to not get created.",
        "NOTE5": "IF raid = true then partitions array is skipped and raid_partitions is used else opposite.",
        "NOTE6": "RAID 1 will not actually occur with the defaults of Vagrant but you can test template modification that occurs for the osd kickstart file which will show how bare-metal will look.",
        "raid": true,
        "partition_option": "ignoredisk --only-use=sda",
        "partitions": [
          {"part": "/boot", "fstype": "xfs", "size": 1024, "options": "--ondisk=sda"},
          {"part": "/", "fstype": "xfs", "size": 10000, "options": "--ondisk=sda"},
          {"part": "/var/lib", "fstype": "xfs", "size": 20000, "options": "--ondisk=sda"},
          {"part": "/opt", "fstype": "xfs", "size": 20000, "options": "--ondisk=sda"},
          {"part": "swap", "fstype": "swap", "size": 10000, "options": ""}
        ],
        "raid_partitions": [
          {"raid": "part raid.01 --size=1024  --ondisk=sde"},
          {"raid": "part raid.02 --size=10000 --ondisk=sde"},
          {"raid": "part raid.03 --size=20000 --ondisk=sde"},
          {"raid": "part raid.04 --size=20000 --ondisk=sde"},
          {"raid": "part raid.05 --size=10000 --ondisk=sde"},
          {"raid": "part raid.06 --size=1024  --ondisk=sdf"},
          {"raid": "part raid.07 --size=10000 --ondisk=sdf"},
          {"raid": "part raid.08 --size=20000 --ondisk=sdf"},
          {"raid": "part raid.09 --size=20000 --ondisk=sdf"},
          {"raid": "part raid.10 --size=10000 --ondisk=sdf"},
          {"raid": "raid /boot      --level=RAID1 --device=md0 --fstype=xfs  raid.01 raid.06"},
          {"raid": "raid /          --level=RAID1 --device=md1 --fstype=xfs  raid.02 raid.07"},
          {"raid": "raid /var/lib   --level=RAID1 --device=md2 --fstype=xfs  raid.03 raid.08"},
          {"raid": "raid /opt       --level=RAID1 --device=md3 --fstype=xfs  raid.04 raid.09"},
          {"raid": "raid swap       --level=RAID1 --device=md4 --fstype=swap raid.05 raid.10"}
        ],
        "NOTE4": "NOTE: Partitions are for OSD nodes. All other partitions are coded into the given ks file.",
        "ports": {
          "http": 80,
          "https": 443,
          "xmlrpc": 25151
        },
        "os": {
          "name": "rhel-7.2",
          "version": "7.2",
          "arch": "x86_64",
          "distro": "rhel-server-7.2-x86_64-dvd.iso",
          "breed": "redhat"
        },
        "redhat": {
          "management": {
            "type": "off",
            "server": "xmlrpc.rhn.redhat.com",
            "key": "cloud-object-store"
          }
        },
        "repo_mirror": false
      },
      "ceph": {
        "cluster": "ceph",
        "repo": {
          "create": true,
          "version": {
            "name": "hammer",
            "branch": "stable",
            "revision": "0.el7",
            "number": "0.94.6",
            "arch": "x86_64"
          }
        },
        "config": {
          "NOTE": "This section is pure key/value. Meaning, the key and value are added to the given location in ceph.conf.",
          "global": {
            "rgw override bucket index max shards": 5
          },
          "NOTE-MON": "Setting the clock drift in vagrant is OK since dev vms can sometimes get out of sync.",
          "NOTE-MON2": "'mon allow pool delete' = false *AFTER* the cluster is stable. Keeps someone from deleting pools :)",
          "NOTE-MON3": "Up the min down reports to 3 in a large cluster. Also, bump up to 600 the 'down out interval' on large clusters.",
          "mon": {
            "mon pg warn max per osd": 0,
            "mon osd full ratio": 0.95,
            "mon osd nearfull ratio": 0.85,
            "mon osd down out interval": 300,
            "mon pg warn max object skew": 10,
            "mon osd min down reporters": 1,
            "mon osd down out interval": 60,
            "clock drift allowed": 5.0
          },
          "radosgw": {
            "cache max file size": 20000000
          }
        },
        "mon": {
          "port": 6789,
          "niceness": -10
        },
        "radosgw": {
          "port": 8080,
          "default_url": "ceph-vm1.ceph.example.com",
          "keystone": {
            "auth": false,
            "admin": {
              "token": "",
              "url": "",
              "port": 35357
            },
            "accepted_roles": "admin Member _member_",
            "token_cache_size": 1000,
            "revocation_interval": 1200
          },
          "rgw_num_rados_handles": 5,
          "civetweb_num_threads": 100
        },
        "osd": {
          "NOTE-RAID": "Cannot really use device sda (vagrant boot disk) as a journal without some work because it takes over the whole drive.",
          "NOTE-RAID-1": "Modify the journal to use different drives to sort of test using journals on a shared OS drive in a bare-metal scenario.",
          "devices": [
            { "data": "/dev/sdb", "data_type": "hdd", "journal": "/dev/sdf", "journal_type": "ssd", "encrypted": false, "status": "" },
            { "data": "/dev/sdc", "data_type": "hdd", "journal": "/dev/sdf", "journal_type": "ssd", "encrypted": false, "status": "" },
            { "data": "/dev/sdd", "data_type": "hdd", "journal": "/dev/sdf", "journal_type": "ssd", "encrypted": false, "status": "" },
            { "data": "/dev/sde", "data_type": "hdd", "journal": "/dev/sdf", "journal_type": "ssd", "encrypted": false, "status": "" }
          ],
          "crush": {
            "NOTE": "update and update_on_start should be opposite of each other.",
            "update": false,
            "update_on_start": true,
            "chooseleaf_type": 1
          },
          "add": [],
          "remove": [],
          "NOTE-DEVICES1": "The 'add' and 'remove' above should reflect the same structure as 'devices' for maintenance.",
          "NOTE-DEVICES2": "The 'add' is only for temporary OSD devices you may need to add. If you need to add long-term devices then put them in 'devices' above and re-run the osd recipe.",
          "niceness": -10,
          "rebalance": false,
          "encrypted": false,
          "size": {
            "max": 3,
            "min": 2
          },
          "journal": {
            "size": 3000,
            "NOTE:": "Calculate min size (if you have space then use it): 2 * (#OSDs in node * 100MB/s) * 5 (default sync interval) = 2 * (3 * 100) * 5 = 3GB "
          }
        },
        "pools": {
          "active": ["radosgw"],
          "crush": {
            "rule": 1
          },
          "erasure_coding": {
            "NOTE": "Failure zone for vagrant is OSD since we don't have a lot of resources. Production it's host.",
            "profiles": [
              {"profile": "bcs", "directory": "/usr/lib64/ceph/erasure-code", "plugin": "jerasure", "force": true, "technique": "reed_sol_van", "ruleset_root": "hdd_erasure", "ruleset_failure_domain": "host", "key_value": {"k": 2, "m": 1}}
            ]
          },
          "radosgw": {
            "federated": {
              "enable": false,
              "zone_instances": [
                {"name": "dev", "port": 8080, "region": "dc1", "url": "s3.dc1.dev.rgw.ceph.example.com"},
                {"name": "prod", "port": 8081, "region": "dc1", "url": "s3.dc1.prod.rgw.ceph.example.com"}
              ],
              "regions": ["dc1"],
              "enable_regions_zones": false,
              "master_zone": "dev",
              "master_zone_port": 8080
            },
            "NOTE-NAME": "If federated then the system generated federated prefix will be added in front of the names below to create a fully federated pool name.",
            "names": [
              ".rgw",
              ".rgw.control",
              ".rgw.gc",
              ".rgw.root",
              ".users.uid",
              ".users.email",
              ".users.swift",
              ".users",
              ".usage",
              ".log",
              ".intent-log",
              ".rgw.buckets",
              ".rgw.buckets.index",
              ".rgw.buckets.extra"
            ],
            "NOTE-POOLS": "pools below replace 'names' and provides data for PG creations. Also, 'actions' portion is EXPERIMENTAL so ignore!",
            "NOTE-POOLS-PERCENT": "data_percent total of the pool should be 100%. IF this is is also a Federated set of pools with multiple tiers then it should still be 100% and the calculation will take those other pools into account.",
            "NOTE-EC-POOLS": "IMPORTANT - ONLY .rgw.buckets (data) pool can be erasure coded. The others must be replicated.",
            "pools": [
              {"name": ".rgw", "data_percent": 0.10, "type": "replicated", "profile": "", "crush_ruleset": 0, "crush_ruleset_name": "hdd_replicated", "actions": [{"action": "create", "pg_num": 32, "type": "erasure", "profile": "bcs"}, {"action": "set", "key": "fast_read", "value": 1}]},
              {"name": ".rgw.control", "data_percent": 0.10, "type": "replicated", "profile": "", "crush_ruleset": 0, "crush_ruleset_name": "hdd_replicated", "actions": [{"action": "create", "pg_num": 32, "type": "erasure", "profile": "bcs"}, {"action": "set", "key": "fast_read", "value": 1}]},
              {"name": ".rgw.gc", "data_percent": 0.10, "type": "replicated", "profile": "", "crush_ruleset": 0, "crush_ruleset_name": "hdd_replicated", "actions": [{"action": "create", "pg_num": 32, "type": "erasure", "profile": "bcs"}, {"action": "set", "key": "fast_read", "value": 1}]},
              {"name": ".rgw.root", "data_percent": 0.10, "type": "replicated", "profile": "", "crush_ruleset": 0, "crush_ruleset_name": "hdd_replicated", "actions": [{"action": "create", "pg_num": 32, "type": "erasure", "profile": "bcs"}, {"action": "set", "key": "fast_read", "value": 1}]},
              {"name": ".users.uid", "data_percent": 0.10, "type": "replicated", "profile": "", "crush_ruleset": 0, "crush_ruleset_name": "hdd_replicated", "actions": [{"action": "create", "pg_num": 32, "type": "erasure", "profile": "bcs"}, {"action": "set", "key": "fast_read", "value": 1}]},
              {"name": ".users.email", "data_percent": 0.10, "type": "replicated", "profile": "", "crush_ruleset": 0, "crush_ruleset_name": "hdd_replicated", "actions": [{"action": "create", "pg_num": 32, "type": "erasure", "profile": "bcs"}, {"action": "set", "key": "fast_read", "value": 1}]},
              {"name": ".users.swift", "data_percent": 0.10, "type": "replicated", "profile": "", "crush_ruleset": 0, "crush_ruleset_name": "hdd_replicated", "actions": [{"action": "create", "pg_num": 32, "type": "erasure", "profile": "bcs"}, {"action": "set", "key": "fast_read", "value": 1}]},
              {"name": ".users", "data_percent": 0.10, "type": "replicated", "profile": "", "crush_ruleset": 0, "crush_ruleset_name": "hdd_replicated", "actions": [{"action": "create", "pg_num": 32, "type": "erasure", "profile": "bcs"}, {"action": "set", "key": "fast_read", "value": 1}]},
              {"name": ".usage", "data_percent": 0.10, "type": "replicated", "profile": "", "crush_ruleset": 0, "crush_ruleset_name": "hdd_replicated", "actions": [{"action": "create", "pg_num": 32, "type": "erasure", "profile": "bcs"}, {"action": "set", "key": "fast_read", "value": 1}]},
              {"name": ".log", "data_percent": 0.10, "type": "replicated", "profile": "", "crush_ruleset": 0, "crush_ruleset_name": "hdd_replicated", "actions": [{"action": "create", "pg_num": 32, "type": "erasure", "profile": "bcs"}, {"action": "set", "key": "fast_read", "value": 1}]},
              {"name": ".intent-log", "data_percent": 0.10, "type": "replicated", "profile": "", "crush_ruleset": 0, "crush_ruleset_name": "hdd_replicated", "actions": [{"action": "create", "pg_num": 32, "type": "erasure", "profile": "bcs"}, {"action": "set", "key": "fast_read", "value": 1}]},
              {"name": ".rgw.buckets", "data_percent": 96.90, "type": "erasure", "profile": "bcs", "crush_ruleset": 1, "crush_ruleset_name": "hdd_erasure", "actions": [{"action": "create", "pg_num": 128, "type": "erasure", "profile": "bcs"}, {"action": "set", "key": "fast_read", "value": 1}]},
              {"name": ".rgw.buckets.index", "data_percent": 1.00, "type": "replicated", "profile": "", "crush_ruleset": 0, "crush_ruleset_name": "hdd_replicated", "actions": [{"action": "create", "pg_num": 32, "type": "erasure", "profile": "bcs"}, {"action": "set", "key": "fast_read", "value": 1}]},
              {"name": ".rgw.buckets.extra", "data_percent": 1.00, "type": "replicated", "profile": "", "crush_ruleset": 0, "crush_ruleset_name": "hdd_replicated", "actions": [{"action": "create", "pg_num": 32, "type": "erasure", "profile": "bcs"}, {"action": "set", "key": "fast_read", "value": 1}]}
            ],
            "NOTE-SETTINGS": "If federated then the settings below apply to the federated pools else the non-federated pools. Chooseleaf - rack is 3.",
            "settings": {
              "pg_num": 128,
              "pgp_num": 128,
              "options": "",
              "force": false,
              "calc": true,
              "size": 3,
              "crush_ruleset": 1,
              "chooseleaf": "host",
              "chooseleaf_type": 1,
              "type": "erasure"
            },
            "remove": {
              "names": []
            }
          },
          "pgs": {
            "NOTE": "Calc below is used ONLY for the initial PG count for each pool. It is NOT meant to be for maintenance of PGs.",
            "calc": {
              "total_osds": 12,
              "target_pgs_per_osd": 200,
              "min_pgs_per_pool": 128,
              "replicated_size": 3,
              "erasure_size": 3,
              "NOTE": "erasure_size is the sum of k+m or 8 + 3 = 11 or 3 + 2 = 5. IF you think the size of the cluster may increase soon then use 200 or 300 for target_pgs_per_osd. IF you believe it will remain the same size (OSDs) for awhile then 100 would be fine."
            },
            "num": 128
          }
        },
        "restapi": {
          "NOTE": "Map the dns to an IP in your production system.",
          "url": "api.ceph.example.com",
          "ip": "10.0.100.21",
          "port": 5080
        }
      },
      "domain_name" : "ceph.example.com",
      "network": {
        "public": {
          "interface": "enp0s8",
          "cidr": [
            "10.0.100.0/24"
          ],
          "mtu": 1500
        },
        "cluster": {
          "interface": "enp0s9",
          "cidr": [
            "10.121.2.0/24"
          ],
          "route": {
            "cidr": ""
          },
          "mtu": 1500
        }
      },
      "dns": {
        "servers": [ "8.8.8.8", "8.8.4.4" ]
      },
      "ntp": {
        "servers": [ "0.pool.ntp.org", "1.pool.ntp.org", "2.pool.ntp.org", "3.pool.ntp.org" ]
      },
      "collectd-plugins": {
        "write_graphite": {
          "node": {
            "id": "graphite",
            "host": "127.0.0.1",
            "port": 2013,
            "prefix": "collectd."
          }
        }
      },
      "zabbix": {
        "repository": "http://repo.zabbix.com/zabbix/2.4/rhel/7/x86_64/",
        "repository_key": "file:///etc/pki/rpm-gpg/RPM-GPG-KEY-ZABBIX",
        "server": "10.0.100.6"
      }
    },
    "chef_client": {
      "server_url": "http://10.0.100.20:4000",
      "cache_path": "/var/chef/cache",
      "backup_path": "/var/chef/backup",
      "validation_client_name": "chef-validator",
      "run_path": "/var/chef"
    }
  }
}
